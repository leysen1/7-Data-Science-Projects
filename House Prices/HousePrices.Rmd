---
title: "Featuring Engineering Practice"
output: 
  html_document:
    toc: true
    toc_depth: 3
author: Machine Learning II
---

#Summary of Changes Made
Feature Engineering: Porch, BldgType_N, MasVnrType_N (all Booleans created from existing variables)
Played around with Information Gain variables vs Chi Squared variables, concluded that IG vars were better at the default 0.05.
Removed a few extreme values in the training set where it made sense and the type was numeric.
Removed GarageArea, TotRmsAbvGrd, and GarageYrBlt has these had high correlations with other explanatory variables.
Added grouped Neighbourhood dummy variables (high, mid high, mid, low). Then removed as made the model worse.
Optimised alpha to 0.27.
Averaged predictions of ridge, lasso and optimised alpha models, to get an overall averaged prediction.

#Results
In the end, my results did not improve on Kaggle from the default code, which had RMSE of 0.1323. My RMSE score on Kaggle is 0.1324.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/charlotteleysen/Google Drive/*PROJECTS/IE/Term 2/Machine Leaning 2/Feature Engineering/Feature Engineering Lab")
# Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_161')
dyn.load('/Library/Java/JavaVirtualMachines/jdk-9.0.1.jdk/Contents/Home/lib/server/libjvm.dylib')
library(ggplot2)
library(plyr)
library(dplyr)
library(moments)
library(glmnet)
library(caret)
library(FSelector)
library(corrplot)
library(data.table)
```


# Introduction
In this first practical session we will make the first contact with the featuring engineering process and its impact in a ML pipeline.
Feature engineering is a very important part of the process of developing prediction models. It is considered, by many authors, an art, and it involves human-driven design and intuition. Feature engineering sessions will try to uncover the most relevant issues that must be addressed, and also provide some guidelines to start building sound feature engineering processes for ML problems. 

The experimental dataset we are going to use is the House Prices Dataset. It includes 79 explanatory variables of residential homes. For more details on the dataset and the competition see <https://www.kaggle.com/c/house-prices-advanced-regression-techniques>.

## What is my goal?
- I have to predict predict the final price of each home (Therefore, this is a regression task)
- I have to use the feature engineering techniques explained in class to transform the dataset.

# Data Reading and preparation
The dataset is offered in two separated fields, one for the training and another one for the test set. Best way of moving forward is to sign in into Kaggle and download them from the 'Data' section.

```{r Load Data}
setwd("/Users/charlotteleysen/Google Drive/*PROJECTS/IE/Term 2/Machine Leaning 2/Feature Engineering/Feature Engineering Lab")
training_data = read.csv(file = file.path("data", "train.csv"))
test_data = read.csv(file = file.path("data", "test.csv"))
```

```{r Check for duplicates}
length(unique(training_data$Id)) == nrow(training_data)
```

There is no duplicates so we remove the Id column
```{r Remove the ID Column}
training_data = training_data[ , -which(names(training_data) %in% c("Id"))]

```

``` {r }

```

Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.

## Hunting NAs
Our dataset is filled with many missing values, therefore, before we can build any predictive model we'll clean our data by filling in all NA's with appropriate values.

Counting columns with null values
```{r NAs discovery}
na.cols <- which(colSums(is.na(training_data)) > 0)
sort(colSums(sapply(training_data[na.cols], is.na)), decreasing = TRUE)
paste('There are', length(na.cols), 'columns with missing values')

```

NA imputation:
```{r Train NA Imputation}
# Alley : NA means "no alley access"
training_data$Alley = factor(training_data$Alley, levels=c(levels(training_data$Alley), "None"))
training_data$Alley[is.na(training_data$Alley)] = "None"

training_data$BedroomAbvGr[is.na(training_data$BedroomAbvGr)] <- 0

# Bsmt : NA for basement features is "no basement"
training_data$BsmtQual = factor(training_data$BsmtQual, levels=c(levels(training_data$BsmtQual), "No"))
training_data$BsmtQual[is.na(training_data$BsmtQual)] = "No"

training_data$BsmtCond = factor(training_data$BsmtCond, levels=c(levels(training_data$BsmtCond), "No"))
training_data$BsmtCond[is.na(training_data$BsmtCond)] = "No"

training_data$BsmtExposure[is.na(training_data$BsmtExposure)] = "No"

training_data$BsmtFinType1 = factor(training_data$BsmtFinType1, levels=c(levels(training_data$BsmtFinType1), "No"))
training_data$BsmtFinType1[is.na(training_data$BsmtFinType1)] = "No"

training_data$BsmtFinType2 = factor(training_data$BsmtFinType2, levels=c(levels(training_data$BsmtFinType2), "No"))
training_data$BsmtFinType2[is.na(training_data$BsmtFinType2)] = "No"

# Fence : NA means "no fence"
training_data$Fence = factor(training_data$Fence, levels=c(levels(training_data$Fence), "No"))
training_data$Fence[is.na(training_data$Fence)] = "No"

# FireplaceQu : NA means "no fireplace"
training_data$FireplaceQu = factor(training_data$FireplaceQu, levels=c(levels(training_data$FireplaceQu), "No"))
training_data$FireplaceQu[is.na(training_data$FireplaceQu)] = "No"

# Garage : NA for garage features is "no garage"
training_data$GarageType = factor(training_data$GarageType, levels=c(levels(training_data$GarageType), "No"))
training_data$GarageType[is.na(training_data$GarageType)] = "No"

training_data$GarageFinish = factor(training_data$GarageFinish, levels=c(levels(training_data$GarageFinish), "No"))
training_data$GarageFinish[is.na(training_data$GarageFinish)] = "No"

training_data$GarageQual = factor(training_data$GarageQual, levels=c(levels(training_data$GarageQual), "No"))
training_data$GarageQual[is.na(training_data$GarageQual)] = "No"

training_data$GarageCond = factor(training_data$GarageCond, levels=c(levels(training_data$GarageCond), "No"))
training_data$GarageCond[is.na(training_data$GarageCond)] = "No"

# LotFrontage : NA most likely means no lot frontage
training_data$LotFrontage[is.na(training_data$LotFrontage)] <- 0

# MasVnrType : NA most likely means no veneer
training_data$MasVnrType[is.na(training_data$MasVnrType)] = "None"
training_data$MasVnrArea[is.na(training_data$MasVnrArea)] <- 0

# MiscFeature : NA = "no misc feature"
training_data$MiscFeature = factor(training_data$MiscFeature, levels=c(levels(training_data$MiscFeature), "No"))
training_data$MiscFeature[is.na(training_data$MiscFeature)] = "No"

# PoolQC : data description says NA means "no pool"
training_data$PoolQC = factor(training_data$PoolQC, levels=c(levels(training_data$PoolQC), "No"))
training_data$PoolQC[is.na(training_data$PoolQC)] = "No"

# Electrical : NA means "UNK"
#training_data$Electrical = factor(training_data$Electrical, levels=c(levels(training_data$Electrical), "UNK"))
training_data$Electrical[is.na(training_data$Electrical)] = "SBrkr"

# GarageYrBlt: It seems reasonable that most houses would build a garage when the house itself was built.
idx <- which(is.na(training_data$GarageYrBlt))
training_data[idx, 'GarageYrBlt'] <- training_data[idx, 'YearBuilt']

na.cols <- which(colSums(is.na(training_data)) > 0)
paste('There are now', length(na.cols), 'columns with missing values')
```

We repeat the process for test_data
```{r Test Inputation}
# Alley : data description says NA means "no alley access"
test_data$Alley = factor(test_data$Alley, levels=c(levels(test_data$Alley), "None"))
test_data$Alley[is.na(test_data$Alley)] = "None"

test_data$BedroomAbvGr[is.na(test_data$BedroomAbvGr)] <- 0

# BsmtQual etc : data description says NA for basement features is "no basement"
test_data$BsmtQual = factor(test_data$BsmtQual, levels=c(levels(test_data$BsmtQual), "No"))
test_data$BsmtQual[is.na(test_data$BsmtQual)] = "No"

test_data$BsmtCond = factor(test_data$BsmtCond, levels=c(levels(test_data$BsmtCond), "No"))
test_data$BsmtCond[is.na(test_data$BsmtCond)] = "No"

test_data$BsmtExposure[is.na(test_data$BsmtExposure)] = "No"

test_data$BsmtFinType1 = factor(test_data$BsmtFinType1, levels=c(levels(test_data$BsmtFinType1), "No"))
test_data$BsmtFinType1[is.na(test_data$BsmtFinType1)] = "No"

test_data$BsmtFinType2 = factor(test_data$BsmtFinType2, levels=c(levels(test_data$BsmtFinType2), "No"))
test_data$BsmtFinType2[is.na(test_data$BsmtFinType2)] = "No"

# Fence : data description says NA means "no fence"
test_data$Fence = factor(test_data$Fence, levels=c(levels(test_data$Fence), "No"))
test_data$Fence[is.na(test_data$Fence)] = "No"

# FireplaceQu : data description says NA means "no fireplace"
test_data$FireplaceQu = factor(test_data$FireplaceQu, levels=c(levels(test_data$FireplaceQu), "No"))
test_data$FireplaceQu[is.na(test_data$FireplaceQu)] = "No"

# GarageType etc : data description says NA for garage features is "no garage"
test_data$GarageType = factor(test_data$GarageType, levels=c(levels(test_data$GarageType), "No"))
test_data$GarageType[is.na(test_data$GarageType)] = "No"

test_data$GarageFinish = factor(test_data$GarageFinish, levels=c(levels(test_data$GarageFinish), "No"))
test_data$GarageFinish[is.na(test_data$GarageFinish)] = "No"

test_data$GarageQual = factor(test_data$GarageQual, levels=c(levels(test_data$GarageQual), "No"))
test_data$GarageQual[is.na(test_data$GarageQual)] = "No"

test_data$GarageCond = factor(test_data$GarageCond, levels=c(levels(test_data$GarageCond), "No"))
test_data$GarageCond[is.na(test_data$GarageCond)] = "No"

# LotFrontage : NA most likely means no lot frontage
test_data$LotFrontage[is.na(test_data$LotFrontage)] <- 0

# MasVnrType : NA most likely means no veneer
test_data$MasVnrType[is.na(test_data$MasVnrType)] = "None"
test_data$MasVnrArea[is.na(test_data$MasVnrArea)] <- 0

# MiscFeature : data description says NA means "no misc feature"
test_data$MiscFeature = factor(test_data$MiscFeature, levels=c(levels(test_data$MiscFeature), "No"))
test_data$MiscFeature[is.na(test_data$MiscFeature)] = "No"

# PoolQC : data description says NA means "no pool"
test_data$PoolQC = factor(test_data$PoolQC, levels=c(levels(test_data$PoolQC), "No"))
test_data$PoolQC[is.na(test_data$PoolQC)] = "No"

na.cols <- which(colSums(is.na(test_data)) > 0)
sort(colSums(sapply(test_data[na.cols], is.na)), decreasing = TRUE)
paste('There are', length(na.cols), 'columns with missing values')


```

##Removing Columns
``` {r Removing Redundant Columns}
#GarageArea - as very correlated with GarageCars
training_data <- training_data[, -which(names(training_data) %in% c("GarageArea"))]
test_data <- test_data[, -which(names(test_data) %in% c("GarageArea"))]

#TotRmsAbvGrd - as correlated with BedroomAbvGr and GrLivArea
training_data <- training_data[, -which(names(training_data) %in% c("TotRmsAbvGrd"))]
test_data <- test_data[, -which(names(test_data) %in% c("TotRmsAbvGrd"))]

#GarageYrBlt - as very correlated with YearBlt
training_data <- training_data[, -which(names(training_data) %in% c("GarageYrBlt"))]
test_data <- test_data[, -which(names(test_data) %in% c("GarageYrBlt"))]


```

## Factorize features
Some numerical features are actually really categories. Therefore we transform the feature from numeric to categorical

```{r Factorize features}

training_data$MSSubClass <- as.factor(training_data$MSSubClass)
training_data$MoSold <- as.factor(training_data$MoSold)

test_data$MSSubClass <- as.factor(test_data$MSSubClass)
test_data$MoSold <- as.factor(test_data$MoSold)
```


## Skewness

If we print the histogram of the target value, we obseve a large skewness in the Target value (i.e., the distribution in not normally distributed).
To solve that we log transform this variable so that it becomes normally distributed. A normally distributed target variable helps in the modeling step (i.e., the finding of the relationship between target and independent variables).
```{r}
# get data frame of SalePrice and log(SalePrice + 1) for plotting
df <- rbind(data.frame(version="log(price+1)",x=log(training_data$SalePrice + 1)),
            data.frame(version="price",x=training_data$SalePrice))

ggplot(data=df) +
  facet_wrap(~version,ncol=2,scales="free_x") +
  geom_histogram(aes(x=x), bins = 50)
```



We therefore transform the target value applying log.
```{r Log transform the target for official scoring}
# Log transform the target for official scoring
training_data$SalePrice <- log1p(training_data$SalePrice)
```


The same "skewness" observed in the target variable also affects other variables. To facilitate the application of the regression model we are going to also eliminate this skewness
For numeric feature with excessive skewness, perform log transformation
```{r}

column_types <- sapply(names(training_data),function(x){class(training_data[[x]])})
numeric_columns <-names(column_types[column_types != "factor"])

# skew of each variable
skew <- sapply(numeric_columns,function(x){skewness(training_data[[x]],na.rm = T)})

# transform all variables above a threshold skewness.
skew <- skew[skew > 0.75]
for(x in names(skew)) {
  training_data[[x]] <- log(training_data[[x]] + 1)
}
```

The same for the test data
```{r}
column_types <- sapply(names(test_data),function(x){class(test_data[[x]])})
numeric_columns <-names(column_types[column_types != "factor"])

skew <- sapply(numeric_columns,function(x){skewness(test_data[[x]],na.rm = T)})
skew <- skew[skew > 0.75]
for(x in names(skew)) {
  test_data[[x]] <- log(test_data[[x]] + 1)
}
```

## Extreme Values
``` {r Extreme Values}
numeric_df <- training_data[,sapply(training_data, is.numeric)]

# boxplot(numeric_df$LotFrontage)
# plot(numeric_df$LotFrontage, numeric_df$SalePrice)

###Remove outliers

for (i in 1:ncol(numeric_df)) {
  Upperbound <- mean(numeric_df[,i]) + sd(numeric_df[,i])*5 
  Lowerbound <- mean(numeric_df[,i]) - sd(numeric_df[,i])*5
  
  #Exclude the variables that have alot of outliers as they may be relevant.
  if (colnames(numeric_df[i]) %in% c("MiscVal", "X3SsnPorch", "LowQualFinSF", "TotalBsmtSF","PoolArea")) {
    # exclude these variables
  } else {
      #Paste the number of values changed for each column
      if (length(numeric_df[numeric_df[,i] > Upperbound,i]) > 0) {
        print(paste("upper bound changed for", colnames(numeric_df[i]), length(numeric_df[numeric_df[,i] > Upperbound,i]), "times"))
        numeric_df[numeric_df[,i] > Upperbound,i] <- Upperbound
      }
      if (length(numeric_df[numeric_df[,i] < Lowerbound,i]) > 0 ) {
          print(paste("lower bound changed for", colnames(numeric_df[i]), length(numeric_df[numeric_df[,i] < Lowerbound,i]), "times"))
          numeric_df[numeric_df[,i] < Lowerbound,i] <- Lowerbound
      }
  }
}

training_data <- cbind(training_data[,sapply(training_data, is.factor)], numeric_df)



```


## Train, Validation Spliting
We are going to split the annotated dataset in training and validation for the later evaluation of our regression models
```{r Train test split}
# I found this function, that is worth to save for future ocasions.
splitdf <- function(dataframe, seed=NULL) {
  if (!is.null(seed)) set.seed(seed)
 	index <- 1:nrow(dataframe)
 	trainindex <- sample(index, trunc(length(index)/1.5))
 	trainset <- dataframe[trainindex, ]
 	testset <- dataframe[-trainindex, ]
 	list(trainset=trainset,testset=testset)
}
splits <- splitdf(training_data, seed=1)
training <- splits$trainset
validation <- splits$testset
```

If we inspect in detail the categorical variables of the dataset, we'll see that some are incomplete: they only have a unique value for all the dataset.
These features are not valuable. Remember the three aspects that a feature should have: informative, <b>discriminative</b> and independent. Incomplete cases are not discriminative at all.
In addition, this might create problems when fitting the regression model

The following code show the incomplete cases
```{r}
## remove incomplete cases
paste("Training set incomplete cases")
sapply(lapply(na.omit(training)[sapply(na.omit(training), is.factor)], droplevels), nlevels)
paste("Validation set incomplete cases")
sapply(lapply(na.omit(validation)[sapply(na.omit(validation), is.factor)], droplevels), nlevels)
paste("Test set incomplete cases")
sapply(lapply(na.omit(test_data)[sapply(na.omit(test_data), is.factor)], droplevels), nlevels)
```

We remove the detected incomplete cases
```{r}
# Remove the Utilities feature from the dataset (It only has one value)
training <- training[,-which(names(training) == "Utilities")]
validation <- validation[,-which(names(validation) == "Utilities")]
test_data <- test_data[,-which(names(test_data) == "Utilities")]

```

#Correlations
```{r Correlations}
# #Take only integer variables
# corr_df <- training_data[,c(sapply(training_data,is.numeric))]
# corr_df <- corr_df[complete.cases(corr_df),]
# 
# corrMat <- cor(corr_df)
# corrplot(corrMat, method = "circle", order = "FPC", type = "lower", tl.cex = 0.7,number.cex = 0.7, cl.cex = 0.7 )
# 
# #Order by highest correlation
# rownames(corrMat)[order(corrMat[ ,"SalePrice"], decreasing = TRUE)]

```

# Feature Engineering
We here start the Feature Engineering.
``` {r Feature Engineering Training Imputation}
#Add a Porch Variable (y/n)
# All porch variables are in square feet, so if the sum is 0, then there is no porch. 
temp <- c("OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "WoodDeckSF")
training$Porch <- rowSums(training[,temp], na.rm = TRUE)
training[,c("Porch")][training[,c("Porch")] >= 1] <- 1
training <- training[ , -which(names(training) %in% c("OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "WoodDeckSF"))]

#Add Garage Variables ()
# training$Garage <- as.character(training$GarageType)
# training[,c("Garage")][training[,c("Garage")] != "No"] <- "1"
# training$Garage <- as.integer(training$Garage)
# training$Garage[is.na(training$Garage)] <- 0

#Add BldgType_N Variable (1Fam + TwnhsE, Rest)
training$BldgType_N <- training$BldgType
training[,c("BldgType_N")][training[,c("BldgType_N")] == "TwnhsE"] <- "1Fam"
training[,c("BldgType_N")][training[,c("BldgType_N")] == "2fmCon"] <- "Duplex"
training[,c("BldgType_N")][training[,c("BldgType_N")] == "Twnhs"] <- "Duplex" 
training$BldgType_N = factor(training$BldgType_N, levels=c("1Fam", "Duplex"))


#Add MasVnrType_N Variable (1Fam + TwnhsE, Rest)
training$MasVnrType_N <- training$MasVnrType
training[,c("MasVnrType_N")][training[,c("MasVnrType_N")] == "None"] <- "BrkCmn"
training[,c("MasVnrType_N")][training[,c("MasVnrType_N")] == "Stone"] <- "BrkFace"
training$MasVnrType_N = factor(training$MasVnrType_N, levels=c("BrkCmn", "BrkFace"))

# #Add Specific Neighbourhoods
# high_hood <- c("NoRidge", "NridgHt","StoneBr")
# mid_high_hood <- c("Timber", "Veenker", "Somerst", "ClearCr", "Crawfor")
# mid_hood <- c("CollgCr","Gilbert", "NWAmes", "SawyerW")
# low_hood <- c("IDOTRR", "MeadowV", "BrDale", "BrkSide", "OldTown", "Edwards")
# 
# training$high_hood[training[,c("Neighborhood")] %in% c("NoRidge", "NridgHt", "StoneBr")] <- 1
# training$high_hood[is.na(training$high_hood)] <- 0
# training$mid_high_hood[training[,c("Neighborhood")] %in% c("Timber", "Veenker", "Somerst", "ClearCr", "Crawfor")] <- 1
# training$mid_high_hood[is.na(training$mid_high_hood)] <- 0
# training$mid_hood[training[,c("Neighborhood")] %in% c("CollgCr","Gilbert", "NWAmes", "SawyerW")] <- 1
# training$mid_hood[is.na(training$mid_hood)] <- 0
# training$low_hood[training[,c("Neighborhood")] %in% c("IDOTRR", "MeadowV", "BrDale", "BrkSide", "OldTown", "Edwards")] <- 1
# training$low_hood[is.na(training$low_hood)] <- 0



#New / Play
# play_df <- data.table(training_data)
# play_df[ ,list(n=.N,meanSalePrice=mean(SalePrice),sdSalePrice=sd(SalePrice)),by='Neighborhood'][order(meanSalePrice, decreasing = TRUE)]
# plot(play_df$SaleType, play_df$SalePrice)


###### Validation Imputation


#Add a Porch Variable (y/n)
# All porch variables are in square feet, so if the sum is 0, then there is no porch. 
temp <- c("OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "WoodDeckSF")
validation$Porch <- rowSums(validation[,temp], na.rm = TRUE)
validation[,c("Porch")][validation[,c("Porch")] >= 1] <- 1
validation <- validation[ , -which(names(validation) %in% c("OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "WoodDeckSF"))]

#Add Garage Variables ()
# validation$Garage <- as.character(validation$GarageType)
# validation[,c("Garage")][validation[,c("Garage")] != "No"] <- "1"
# validation$Garage <- as.integer(validation$Garage)
# validation$Garage[is.na(validation$Garage)] <- 0

#Add BldgType_N Variable (1Fam + TwnhsE, Rest)
validation$BldgType_N <- validation$BldgType
validation[,c("BldgType_N")][validation[,c("BldgType_N")] == "TwnhsE"] <- "1Fam"
validation[,c("BldgType_N")][validation[,c("BldgType_N")] == "2fmCon"] <- "Duplex"
validation[,c("BldgType_N")][validation[,c("BldgType_N")] == "Twnhs"] <- "Duplex" 
validation$BldgType_N = factor(validation$BldgType_N, levels=c("1Fam", "Duplex"))

#Add MasVnrType Variable (1Fam + TwnhsE, Rest)
validation$MasVnrType_N <- validation$MasVnrType
validation[,c("MasVnrType_N")][validation[,c("MasVnrType_N")] == "None"] <- "BrkCmn"
validation[,c("MasVnrType_N")][validation[,c("MasVnrType_N")] == "Stone"] <- "BrkFace"
validation$MasVnrType_N = factor(validation$MasVnrType_N, levels=c("BrkCmn", "BrkFace"))
# 
# #Add Specific Neighbourhoods
# validation$high_hood[validation[,c("Neighborhood")] %in% c("NoRidge", "NridgHt", "StoneBr")] <- 1
# validation$high_hood[is.na(validation$high_hood)] <- 0
# validation$mid_high_hood[validation[,c("Neighborhood")] %in% c("Timber", "Veenker", "Somerst", "ClearCr", "Crawfor")] <- 1
# validation$mid_high_hood[is.na(validation$mid_high_hood)] <- 0
# validation$mid_hood[validation[,c("Neighborhood")] %in% c("CollgCr","Gilbert", "NWAmes", "SawyerW")] <- 1
# validation$mid_hood[is.na(validation$mid_hood)] <- 0
# validation$low_hood[validation[,c("Neighborhood")] %in% c("IDOTRR", "MeadowV", "BrDale", "BrkSide", "OldTown", "Edwards")] <- 1
# validation$low_hood[is.na(validation$low_hood)] <- 0
# 
# 

### Move Sales Price to end of tables
training <- training[,c(which(colnames(training)!="SalePrice"),which(colnames(training)=="SalePrice"))]
validation <- validation[,c(which(colnames(validation)!="SalePrice"),which(colnames(validation)=="SalePrice"))]

```

``` {r Feature Engineering Test Imputation}
#Add a Porch Variable (y/n)
# All porch variables are in square feet, so if the sum is 0, then there is no porch. 
temp <- c("OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "WoodDeckSF")
test_data$Porch <- rowSums(test_data[,temp], na.rm = TRUE)
test_data[,c("Porch")][test_data[,c("Porch")] >= 1] <- 1
test_data <- test_data[ , -which(names(test_data) %in% c("OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "WoodDeckSF"))]

#Add Garage Variables ()
# test_data$Garage <- as.character(test_data$GarageType)
# test_data[,c("Garage")][test_data[,c("Garage")] != "No"] <- "1"
# test_data$Garage <- as.integer(test_data$Garage)
# test_data$Garage[is.na(test_data$Garage)] <- 0

#Add BldgType_N Variable (1Fam + TwnhsE, Rest)
test_data$BldgType_N <- test_data$BldgType
test_data[,c("BldgType_N")][test_data[,c("BldgType_N")] == "TwnhsE"] <- "1Fam"
test_data[,c("BldgType_N")][test_data[,c("BldgType_N")] == "2fmCon"] <- "Duplex"
test_data[,c("BldgType_N")][test_data[,c("BldgType_N")] == "Twnhs"] <- "Duplex" 
test_data$BldgType_N = factor(test_data$BldgType_N, levels=c("1Fam", "Duplex"))

#Add MasVnrType Variable (1Fam + TwnhsE, Rest)
test_data$MasVnrType_N <- test_data$MasVnrType
test_data[,c("MasVnrType_N")][test_data[,c("MasVnrType_N")] == "None"] <- "BrkCmn"
test_data[,c("MasVnrType_N")][test_data[,c("MasVnrType_N")] == "Stone"] <- "BrkFace"
test_data$MasVnrType_N = factor(test_data$MasVnrType_N, levels=c("BrkCmn", "BrkFace"))
# 
# #Add Specific Neighbourhoods
# test_data$high_hood[test_data[,c("Neighborhood")] %in% c("NoRidge", "NridgHt", "StoneBr")] <- 1
# test_data$high_hood[is.na(test_data$high_hood)] <- 0
# test_data$mid_high_hood[test_data[,c("Neighborhood")] %in% c("Timber", "Veenker", "Somerst", "ClearCr", "Crawfor")] <- 1
# test_data$mid_high_hood[is.na(test_data$mid_high_hood)] <- 0
# test_data$mid_hood[test_data[,c("Neighborhood")] %in% c("CollgCr","Gilbert", "NWAmes", "SawyerW")] <- 1
# test_data$mid_hood[is.na(test_data$mid_hood)] <- 0
# test_data$low_hood[test_data[,c("Neighborhood")] %in% c("IDOTRR", "MeadowV", "BrDale", "BrkSide", "OldTown", "Edwards")] <- 1
# test_data$low_hood[is.na(test_data$low_hood)] <- 0
# 
# ### Move Sales Price to end of table
# test_data <- test_data[,c(which(colnames(test_data)!="SalePrice"),which(colnames(test_data)=="SalePrice"))]
# 


```

## Filtering Methods
We will rank the features according to their predictive power according to the methodologies seen in class: the Chi Squared Independence test and the Information Gain.

#### Full Model
We first fit a lm model with all the features to have a baseline to evaluate the impact of the feature engineering.
```{r Full Regression model, message=FALSE, warning=FALSE}

set.seed(121)
train_control_config <- trainControl(method = "repeatedcv", 
                       number = 5, 
                       repeats = 1,
                       returnResamp = "all")

full.lm.mod <- train(SalePrice ~ ., data = training, 
               method = "lm", 
               metric = "RMSE",
               preProc = c("center", "scale"),
               trControl=train_control_config)

for (x in names(validation)) {
  full.lm.mod$xlevels[[x]] <- union(full.lm.mod$xlevels[[x]], levels(validation[[x]]))
}
full.lm.mod.pred <- predict(full.lm.mod, validation[,-ncol(validation)])
full.lm.mod.pred[is.na(full.lm.mod.pred)] <- 0

my_data=as.data.frame(cbind(predicted=full.lm.mod.pred,observed=validation$SalePrice))

ggplot(my_data,aes(predicted,observed))+
  geom_point() + geom_smooth(method = "lm") +
  labs(x="Predicted") +
  ggtitle('Linear Model')

paste("Full Linear Regression RMSE = ", sqrt(mean((full.lm.mod.pred - validation$SalePrice)^2)))
```

### Chi-squared Selection
Making use of the `FSelector` package <https://cran.r-project.org/web/packages/FSelector/FSelector.pdf>, rank the features according to the Chi Squared value. If you've problems with this package (some of us have problems with it), do some research to find another packages that will provide the Chi squared selection.

Does it make sense to remove some features? Is so, do it! <b>(Tip: Sure it does)</b>
```{r}
weights<- data.frame(chi.squared(SalePrice~., training))
weights$feature <- rownames(weights)
weights[order(weights$attr_importance, decreasing = TRUE),]
chi_squared_features <- weights$feature[weights$attr_importance >= 0.1]
```
  
#### Evaluation
Evaluate the impact (in terms of RMSE) of the feature selection.
To that end, execute the previous LM model taking as input the filtered training set
```{r Chi-Squared Regression, message=FALSE, warning=FALSE}
chi_squared.lm.mod <- train(SalePrice ~ ., data = training[append(chi_squared_features, "SalePrice")], 
               method = "lm", 
               metric = "RMSE",
               preProc = c("center", "scale"),
               trControl=train_control_config)

for (x in names(validation)) {
  chi_squared.lm.mod$xlevels[[x]] <- union(chi_squared.lm.mod$xlevels[[x]], levels(validation[[x]]))
}
chi_squared.lm.mod.pred <- predict(chi_squared.lm.mod, validation[,-ncol(validation)])
chi_squared.lm.mod.pred[is.na(chi_squared.lm.mod.pred)] <- 0

my_data=as.data.frame(cbind(predicted=chi_squared.lm.mod.pred,observed=validation$SalePrice))

ggplot(my_data,aes(predicted,observed))+
  geom_point() + geom_smooth(method = "lm") +
  labs(x="Predicted") +
  ggtitle('Linear Model')

paste("Chi-Squared Filtered Linear Regression RMSE = ", sqrt(mean((chi_squared.lm.mod.pred - validation$SalePrice)^2)))
```

### Information Gain Selection
Let's experiment now with Information Gain Selection.
Making also use of the `FSelector` package <https://cran.r-project.org/web/packages/FSelector/FSelector.pdf>, rank the features according to their Information Gain and filter those which you consider, according to the IG value.

Again, there're more alternatives to compute the IG.
```{r}
IG_weight <- 0.05
weights<- data.frame(information.gain(SalePrice~., training))
weights$feature <- rownames(weights)
weights[order(weights$attr_importance, decreasing = TRUE),]
information_gain_features <- weights$feature[weights$attr_importance >= IG_weight]
```

#### Evaluation
Evaluate the impact of the IG selection in the model performance
```{r Information Gain Regression Model, message=FALSE, warning=FALSE}
ig.lm.mod <- train(SalePrice ~ ., data = training[append(information_gain_features, "SalePrice")], 
               method = "lm", 
               metric = "RMSE",
               preProc = c("center", "scale"),
               trControl=train_control_config)

for (x in names(validation)) {
  ig.lm.mod$xlevels[[x]] <- union(ig.lm.mod$xlevels[[x]], levels(validation[[x]]))
}
ig.lm.mod.pred <- predict(ig.lm.mod, validation[,-ncol(validation)])
ig.lm.mod.pred[is.na(ig.lm.mod.pred)] <- 0

my_data=as.data.frame(cbind(predicted=ig.lm.mod.pred,observed=validation$SalePrice))

ggplot(my_data,aes(predicted,observed))+
  geom_point() + geom_smooth(method = "lm") +
  labs(x="Predicted") +
  ggtitle('Linear Model')

paste("IG Filtered Linear Regression RMSE = ", sqrt(mean((ig.lm.mod.pred - validation$SalePrice)^2)))
```

Using the result of the evaluation, filter the dataset (according to the method and cutoff that you decide)

#### Change Use of Variables from Information Gain to Chi Squared Variables
```{r Change of variables}
chosen_variables <- information_gain_features
#chosen_variables <- chi_squared_features


```



Based on these results, we filter the training and validation set with the Information Gain features.
```{r}
training <- training[append(chosen_variables, "SalePrice")]
validation <- validation[append(c(chosen_variables), "SalePrice")]
```

## Wrapper Methods
Let us experiment now with Wrapper Methods. In particular, we are going to apply Forward Stepwise Selection Methods to find the best feature combination for this dataset.


### Stepwise


#### Backward Stepwise
`caret` package provides a useful and easy way of experimenting with stepwise selection. Try it to know what a wrapper method suggests as the best possible subset of features and compare your results with the baseline.
```{r Backward Stepwise, message=FALSE, warning=FALSE}

train_control_config_4_stepwise <- trainControl(method = "none")
# 
# backward.lm.mod <- train(SalePrice ~ ., data = training,
#                method = "glmStepAIC",
#                direction = "backward",
#                trace = FALSE,
#                metric = "RMSE",
#                steps = 15,
#                preProc = c("center", "scale"),
#                trControl=train_control_config_4_stepwise)
```

Printout only the selected features.
```{r}
# paste("Features Selected" ,backward.lm.mod$finalModel$formula[3])
```

Comput the RMSE of the selected model
```{r}
# for (x in names(validation)) {
#   backward.lm.mod$xlevels[[x]] <- union(backward.lm.mod$xlevels[[x]], levels(validation[[x]]))
# }
# backward.lm.mod.pred <- predict(backward.lm.mod, validation[,-ncol(validation)])
# backward.lm.mod.pred[is.na(backward.lm.mod.pred)] <- 0
# 
# 
# 
# paste("Forward Linear Regression RMSE = ", sqrt(mean((backward.lm.mod.pred - validation$SalePrice)^2))) 
# 
# my_data=as.data.frame(cbind(predicted=backward.lm.mod.pred,observed=validation$SalePrice)) 
# ggplot(my_data,aes(predicted,observed))
# geom_point() + geom_smooth(method = "lm")
# labs(x="Predicted")
# ggtitle('Linear Model')
```

#### Forward Stepwise

Try the same with forward stepwise.

```{r Forward Stepwise, message=FALSE, warning=FALSE}
forward.lm.mod <- step(glm(training$SalePrice ~ 1, data = training[,-ncol(training)]), direction = "forward", scope=formula(glm(training$SalePrice ~ ., data = training[,-ncol(training)])))

forward.lm.mod <- train(x = training[-ncol(training)], y = training$SalePrice,
                                       method = "glmStepAIC", 
                                       direction = "forward",
                                       steps = 10,
                                       trace=FALSE,
                                       metric = "RMSE",
                                       preProc = c("center", "scale"),
                        trControl=train_control_config_4_stepwise)


```

Printout only the selected features.
```{r}
paste("Features Selected" ,forward.lm.mod$finalModel$formula[3])

```

Compute the new RMSE

```{r}
for (x in names(validation)) {
  forward.lm.mod$xlevels[[x]] <- union(forward.lm.mod$xlevels[[x]], levels(validation[[x]]))
}

forward.lm.mod.pred <- predict(forward.lm.mod, validation[,-which(names(validation) %in% c("SalePrice"))])
forward.lm.mod.pred[is.na(forward.lm.mod.pred)] <- 0



paste("Forward Linear Regression RMSE = ", sqrt(mean((forward.lm.mod.pred - validation$SalePrice)^2)))

my_data=as.data.frame(cbind(predicted=forward.lm.mod.pred,observed=validation$SalePrice))
ggplot(my_data,aes(predicted,observed))+
  geom_point() + geom_smooth(method = "lm") +
  labs(x="Predicted") +
  ggtitle('Linear Model')
```


```{r}
forward_features <- c("OverallQual", "Neighborhood", "GrLivArea", "BsmtFinSF1" ,"MSSubClass", "OverallCond", "GarageCars", "YearBuilt", "LotArea", "MSZoning")
```


## Embedded
Finally, we will experiment with embedded methods. 
In particular we are going to focus on Ridge and  Lasso Regularization.

### Ridge Regression
For this exercise, we are going to make use of the <a href="https://cran.r-project.org/web/packages/glmnet/index.html">`glmnet`</a> library. Take a look to the library and fit a glmnet model for Ridge Regression, using the grid of lambda values provided.
```{r Ridge Regression, warning=FALSE}
lambdas <- 10^seq(-2, 3, by = .1)
ridge.mod <- glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = 0, lambda = lambdas)
```

#### Evaluation
Plotting the RMSE for the different lambda values, we can see the impact of this parameter in the model performance.
Small values seem to work better for this dataset
```{r Ridge Evaluation}
RMSE = numeric(length(lambdas))
for (i in seq_along(lambdas)){
  ridge.pred=predict(ridge.mod, s=lambdas[i], data.matrix(validation[,-ncol(validation)]))
  RMSE[i] <- sqrt(mean((ridge.pred - validation$SalePrice)^2))
}
plot(lambdas, RMSE, main="Ridge", log="x", type = "b")

```


##### Cross Validation
Making use of cv.glmnet <https://www.rdocumentation.org/packages/glmnet/versions/2.0-12/topics/cv.glmnet>, create a cross-validated Ridge Regression Model for the provided lambdas.

Plotting again the error, CV give us a better understanding on the impact of lambda in the model performance
```{r}
ridge.cv_fit <- cv.glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = 0, lambda = lambdas)
plot(ridge.cv_fit)
```

<b>Interpretation:</b>

1. The plot shows the MSE (red dots) for the provided lambda values (included in the grid).
2. The confidence intervals represent error estimates for the RSE, computed using CV. 
3. The vertical lines show the locations of lambda.min (lambda that achives the best MSE) and lambda.1se (the largest lambda value within 1 standard error of lambda.min. Using lambda.1se hedges against overfitting by selecting a larger lambda value than the min).
4. The numbers across the top are the number of nonzero coefficient estimates.

Select the best lambda form the CV model, use it to predict the target value of the validation set and evaluate the results (in terms of RMSE)
```{r}
bestlam <- ridge.cv_fit$lambda.min
paste("Best Lambda value from CV=", bestlam)
ridge.pred=predict(ridge.mod, s=bestlam, data.matrix(validation[,-ncol(validation)]))
paste("RMSE for lambda ", bestlam, " = ", sqrt(mean((ridge.pred - validation$SalePrice)^2)))
```


Select the λ1se value from the CV model to predict on the validation set
```{r}
lam1se <- ridge.cv_fit$lambda.1se
paste("Lambda 1se value from CV=", lam1se)
ridge.pred=predict(ridge.mod, s=lam1se, data.matrix(validation[,-ncol(validation)]))
paste("RMSE for lambda ", lam1se, " = ", sqrt(mean((ridge.pred - validation$SalePrice)^2)))
```
As you can see, the result is almost the same, but the 1se value is less prone to overfitting

Let's plot the predictions against the actual values to have an idea of the model performance
```{r}
# Plot important coefficients
my_data=as.data.frame(cbind(predicted=ridge.pred,observed=validation$SalePrice))

ggplot(my_data,aes(my_data["1"],observed))+
  geom_point()+geom_smooth(method="lm")+
  scale_x_continuous(expand = c(0,0)) +
  labs(x="Predicted") +
  ggtitle('Ridge')
```

Rank the variables according to the importance attributed by the model
```{r}

# Print, plot variable importance
imp <- varImp(ridge.mod, lambda = bestlam)
names <- rownames(imp)[order(imp$Overall, decreasing=TRUE)]
importance <- imp[names,]

data.frame(row.names = names, importance)

```

### Lasso Regresion
Using again the <a href="https://cran.r-project.org/web/packages/glmnet/index.html">`glmnet`</a> library, fit a Lasso Regression (take a look to the alpha parameter) using the grid of lambda values provided.

#### Evaluation
Plot the RMSE for the different lambda values and Explain the results.
```{r}
lambdas <- 10^seq(-3, 3, by = .1)

lasso.cv_fit <- cv.glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = 1, lambda = lambdas)
plot(lasso.cv_fit)
```


<b>Interpretation:</b>
As said in class, In contrast to Ridge Regression, Lasso Regression performs feature selection (it is forcing the coefficients to be 0), as you can see in the top numbers in the plot.


Select the best lambda form the CV model, use it to predict the target value of the validation set and evaluate the results (in terms of RMSE)
```{r}
bestlam <- lasso.cv_fit$lambda.min
paste("Best Lambda value from CV=", bestlam)
lasso.mod <- glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = 1, lambda = lambdas)
lasso.pred=predict(lasso.mod, s=bestlam, data.matrix(validation[,-ncol(validation)]))
paste("RMSE for lambda ", bestlam, " = ", sqrt(mean((lasso.pred - validation$SalePrice)^2)))
```

Select the λ1se value from the CV model to predict on the validation set
```{r}
lam1se <- lasso.cv_fit$lambda.1se
paste("Lambda 1se value from CV=", lam1se)
lasso.mod <- glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = 1, lambda = lambdas)
lasso.pred=predict(lasso.mod, s=lam1se, data.matrix(validation[,-ncol(validation)]))
paste("RMSE for lambda ", lam1se, " = ", sqrt(mean((lasso.pred - validation$SalePrice)^2)))
```

Predictions against the actual values 
```{r}
# Plot important coefficients
my_data=as.data.frame(cbind(predicted=lasso.pred,observed=validation$SalePrice))

ggplot(my_data,aes(my_data["1"],observed))+
  geom_point()+geom_smooth(method="lm")+
  scale_x_continuous(expand = c(0,0)) +
  labs(x="Predicted") +
  ggtitle('Lasso')
```

Variable importance
```{r}
# Print, plot variable importance
imp <- varImp(lasso.mod, lambda = bestlam)
names <- rownames(imp)[order(imp$Overall, decreasing=TRUE)]
importance <- imp[names,]

data.frame(row.names = names, importance)

```

Variables selected by the lasso model (only those with importance larger than 0)
```{r}
filtered_names <- rownames(imp)[order(imp$Overall, decreasing=TRUE)][1:28]
print(filtered_names)
```


# Prediction on the test data

```{r}

log_prediction <- predict(lasso.cv_fit,  s=lasso.cv_fit$lambda.min, newx = data.matrix(test_data[chosen_variables]))
actual_pred <- exp(log_prediction)-1
hist(actual_pred)
submit <- data.frame(Id=test_data$Id,SalePrice=actual_pred)
colnames(submit) <-c("Id", "SalePrice")

submit$SalePrice[is.na(submit$SalePrice)] <- 0
replace_value_for_na <- sum(na.omit(submit$SalePrice))/(nrow(submit) - sum(submit$SalePrice == 0))
submit$SalePrice[submit$SalePrice == 0] <- replace_value_for_na

write.csv(submit,file="lasso_information_gain.csv",row.names=F)
```

## Playing with Alpha
``` {r Playinng with Alpha}
alpha = 0.27
lambdas <- 10^seq(-3, 3, by = .1)
# Run the model
mix.cv_fit <- cv.glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = alpha, lambda = lambdas)

# Find the best lambda
bestlam <- mix.cv_fit$lambda.min
paste("Best Lambda value from CV=", bestlam)

#RMSE on training
mix.mod <- glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = alpha, lambda = lambdas)
mix.pred=predict(mix.mod, s=bestlam, data.matrix(validation[,-ncol(validation)]))
paste("RMSE for lambda ", bestlam, " = ", sqrt(mean((mix.pred - validation$SalePrice)^2)))

#Select the λ1se value from the CV model to predict on the validation set
lam1se <- mix.cv_fit$lambda.1se
paste("Lambda 1se value from CV=", lam1se)

#RMSE on validation
mix.mod <- glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = alpha, lambda = lambdas)
mix.pred=predict(mix.mod, s=lam1se, data.matrix(validation[,-ncol(validation)]))
paste("RMSE for lambda ", lam1se, " = ", sqrt(mean((mix.pred - validation$SalePrice)^2)))


```

## Multiple Predictions to Combine
``` {r Predicting Multiple Combined}
# Lasso a =  1
log_prediction_lasso <- predict(lasso.cv_fit,  s=lasso.cv_fit$lambda.min, newx = data.matrix(test_data[chosen_variables]))
actual_pred_lasso <- exp(log_prediction_lasso)-1

#Ridge a = 0
log_prediction_ridge <- predict(ridge.cv_fit,  s=ridge.cv_fit$lambda.min, newx = data.matrix(test_data[chosen_variables]))
actual_pred_ridge <- exp(log_prediction_ridge)-1

#Mix a 
log_prediction_mix <- predict(mix.cv_fit,  s=mix.cv_fit$lambda.min, newx = data.matrix(test_data[chosen_variables]))
actual_pred_mix <- exp(log_prediction_mix)-1

# Calculate the average prediction value
mix_predictions <- data.frame(cbind(actual_pred_lasso,actual_pred_ridge,actual_pred_mix))
colnames(mix_predictions) <- c("lasso","ridge","mix")
mix_predictions$average <- rowMeans(mix_predictions, na.rm = TRUE)
sum(is.na(mix_predictions$average))

#Removing nas in predictions - average out.
mix_predictions[is.na(mix_predictions$lasso), "lasso"] <- mean(mix_predictions$lasso, na.rm = TRUE)
mix_predictions[is.na(mix_predictions$ridge), "ridge"] <- mean(mix_predictions$ridge, na.rm = TRUE)
mix_predictions[is.na(mix_predictions$mix), "mix"] <- mean(mix_predictions$mix, na.rm = TRUE)
mix_predictions[is.na(mix_predictions$average), "average"] <- mean(mix_predictions$average, na.rm = TRUE)

# Combine the models
#final_df <- data.frame(cbind(test_data$Id,mix_predictions$average))
  #Just take the mix model
final_df <- data.frame(cbind(test_data$Id,mix_predictions$average))
colnames(final_df) <- c("Id", "SalePrice")
rownames(final_df) <- c()

setwd("/Users/charlotteleysen/Google Drive/*PROJECTS/IE/Term 2/Machine Leaning 2/Feature Engineering/Feature Engineering Lab/HousePrice Kaggle")
write.csv(final_df, file = "HP_submission_average.csv", sep = ",",row.names = F)

## The RMSE for each model
paste("Chi-Squared Filtered Linear Regression RMSE = ", sqrt(mean((chi_squared.lm.mod.pred - validation$SalePrice)^2)))
paste("IG Filtered Linear Regression RMSE = ", sqrt(mean((ig.lm.mod.pred - validation$SalePrice)^2)))

#paste("Backward Linear Regression RMSE = ", sqrt(mean((backward.lm.mod.pred - validation$SalePrice)^2)))
paste("Forward Linear Regression RMSE = ", sqrt(mean((forward.lm.mod.pred - validation$SalePrice)^2)))

#Ridge
paste("RMSE for lambda ", lam1se, " = ", sqrt(mean((ridge.pred - validation$SalePrice)^2)))
#Lasso
paste("RMSE for lambda ", lam1se, " = ", sqrt(mean((lasso.pred - validation$SalePrice)^2)))
#Mix
paste("RMSE for lambda ", lam1se, " = ", sqrt(mean((mix.pred - validation$SalePrice)^2)))

```